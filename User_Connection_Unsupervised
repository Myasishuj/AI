import pandas as pd
import numpy as np
from datetime import datetime
import unicodedata
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors

# --- Helpers ---
def normalize(s: str) -> str:
    return (
        unicodedata.normalize("NFKD", s)
        .encode("ascii", "ignore")
        .decode("ascii")
        .lower()
        .strip()
    )

# --- 1. Load and preprocess data ---
INPUT_CSV = "SocialMediaUsersDataset_with_all_coordinates.csv"
try:
    df = pd.read_csv(INPUT_CSV)
    print(f"Loaded '{INPUT_CSV}'. Shape: {df.shape}")
except FileNotFoundError:
    print(f"Error: File '{INPUT_CSV}' not found.")
    exit()

# Remove duplicates
initial_rows = len(df)
df.drop_duplicates(inplace=True)
print(f"Removed {initial_rows - len(df)} duplicates. New shape: {df.shape}")

# Optional sampling (disable for full evaluation)
USE_SAMPLE = False
SAMPLE_FRACTION = 0.1
if USE_SAMPLE:
    df = df.sample(frac=SAMPLE_FRACTION, random_state=42).reset_index(drop=True)
    print(f"Sampled to {df.shape}")

# Age from DOB
df['DOB'] = pd.to_datetime(df['DOB'], errors='coerce')
calculated_age = ((pd.Timestamp(datetime.now()) - df['DOB']).dt.days / 365.25)
df['age'] = calculated_age.fillna(calculated_age.mean() if not calculated_age.isnull().all() else 0)
print(f"Age calculated. Range: {df['age'].min():.2f}–{df['age'].max():.2f}")

# Coordinates
df['latitude'] = df['latitude'].fillna(df['latitude'].mean() if not df['latitude'].isnull().all() else 0.0)
df['longitude'] = df['longitude'].fillna(df['longitude'].mean() if not df['longitude'].isnull().all() else 0.0)
coords = df[['latitude', 'longitude']].values

# Interests normalization
df['Interests'] = df['Interests'].astype(str).apply(lambda x: normalize(x.replace("'", " ")))
interests = df['Interests']

# --- 2. Feature extraction ---
tfidf = TfidfVectorizer(max_features=100, ngram_range=(1, 2))
X_text = tfidf.fit_transform(interests)

numeric_scaler = StandardScaler()
X_num = numeric_scaler.fit_transform(np.column_stack([df['age'].values.reshape(-1, 1), coords]))

X_combined = np.hstack([X_text.toarray(), X_num])
pca = PCA(n_components=50)
X_reduced = pca.fit_transform(X_combined)

# --- 3. NearestNeighbors model ---
n_neighbors = 3
nn_model = NearestNeighbors(n_neighbors=n_neighbors + 1, metric='cosine')
nn_model.fit(X_reduced)
distances, indices = nn_model.kneighbors(X_reduced)

# --- 4. Friend recommendation function ---
def get_friend_recommendations(user_id, num_recommendations=3, initial_threshold=0.85, min_threshold=0.70, step=0.05):
    if user_id < 0 or user_id >= len(df):
        return []

    user_distances = distances[user_id]
    user_indices = indices[user_id]

    current_threshold = initial_threshold
    while current_threshold >= min_threshold:
        recs = []
        for i in range(1, len(user_indices)):  # skip self (index 0)
            neighbor_idx = user_indices[i]
            similarity = 1 - user_distances[i]
            if similarity >= current_threshold:
                recs.append((df.loc[neighbor_idx, 'Name'], similarity))

        if len(recs) >= num_recommendations:
            recs.sort(key=lambda x: x[1], reverse=True)
            return recs[:num_recommendations]

        current_threshold -= step  # Lower the bar and retry

    # Fallback: return top matches regardless of threshold
    fallback_recs = []
    for i in range(1, len(user_indices)):
        neighbor_idx = user_indices[i]
        similarity = 1 - user_distances[i]
        fallback_recs.append((df.loc[neighbor_idx, 'Name'], similarity))

    fallback_recs.sort(key=lambda x: x[1], reverse=True)
    return fallback_recs[:num_recommendations]

# --- 5. Evaluate all users ---
print("\n--- Evaluating all users ---")
all_similarities = []
recommendation_column = []

for user_id in range(len(df)):
    recs = get_friend_recommendations(user_id, num_recommendations=3)
    recommended_names = [name for name, sim in recs]
    recommendation_column.append(", ".join(recommended_names))

    if recs:
        sims = [sim for _, sim in recs]
        avg_sim = sum(sims) / len(sims)
        all_similarities.extend(sims)
        print(f"User {df.loc[user_id, 'Name']} (ID {user_id}): Avg sim = {avg_sim:.3f}")
    else:
        print(f"User {df.loc[user_id, 'Name']} (ID {user_id}): No strong matches.")

# Add column
df['RecommendedFriends'] = recommendation_column

# --- 6. Print summary ---
print("\n--- Summary ---")
print(f"Evaluated {len(df)} users.")
print(f"Similarity threshold: > 0.85")
if all_similarities:
    print(f"Overall average similarity: {np.mean(all_similarities):.3f}")
    print(f"Similarity range: {min(all_similarities):.3f} – {max(all_similarities):.3f}")
else:
    print("No similarities found above threshold.")

# --- 7. Save updated dataset ---
OUTPUT_CSV = "SocialMediaUsers_with_Recommendations.csv"
df.to_csv(OUTPUT_CSV, index=False)
print(f"Saved updated DataFrame to '{OUTPUT_CSV}'")
